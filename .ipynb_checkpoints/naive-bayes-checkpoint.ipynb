{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pràctiques de Nous Usos de la Informàtica"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes i Classificació"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>ENTREGA: </b>\n",
    "El dia límit per a l'entrega d'aquesta pràctica és el dia <b>6 de Desembre a les 23.55h</b>\n",
    "\n",
    "<b>Format de l'entrega</b>\n",
    "L'entrega s'efecturà mitjançant el campus virtual. S'ha de penjar un arxiu per grup. El nom del fitxer ha de seguir el següent patró:\n",
    "NUI_2_PrimeralletranomCognomMembre1_PrimeralletranomCognomMembre2.iypnb\n",
    "\n",
    "Exemple: <br>\n",
    "Membre 1: Maria del Carme Vilà<br>\n",
    "Membre 2: Francesc Castell<br>\n",
    "\n",
    "Nom de l'arxiu: <b>NUI_2_MVila_FCastell.ipynb</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Què s’ha de fer?</b><br>\n",
    "Volem classificar notícies corresponents al diari New York Times segons de quin tòpic parlin principalment. A partir  de totes les notíces que tenim guardades en un fitxer .csv, crearem un vector de característiques que ens descrigui  cada notícia. Finalment desenvoluparem un classificador probabilístic del tipus Naive Bayes que ens permeti identificar a quin tòpic pertany una notícia donada segons les característiques triades.<br>\n",
    "\n",
    "<b>Quina és la idea del sistema de classificació que s’ha de desenvolupar?</b><br>\n",
    "El classificador és un concepte de l'aprenentatge automàtic supervisat. L'objectiu del classificador és donat un vector de característiques que descriuen els objectes que es volen classificar indicar a quina categoria o classe pertanyen d'entre un conjunt predeterminat. El procés de classificació consta de dues parts: (a) el procés d'aprenentatge i (b) el procés d'explotació o testeig. El procés d'aprenentatge rep exemples de parelles (x,y) on x són les característiques, usualment nombres reals, i y és la categoria a la que pertanyen. Aquest conjunt se'l coneix com a conjunt d'entrenament i ens servirà per trobar una funció y=h(x) que donada una x m'indiqui quina és la y. Per altra banda el procés de testeig aplica la funció h(x) apresa a l'entrenament a una nova descripció per veure quina categoria li correspon.</br>\n",
    "\n",
    "<b>Classificació i llenguatge natural</b><br>\n",
    "La descripció dels exemples en característiques és el punt més crític de tot sistema d'aprenentatge automàtic. Una de les representacions més simples per tal de descriure un text és la representació bag-of-words. Aquesta representació converteix un text en un vector de N paraules. Consisteix en seleccionar un conjunt d'N paraules i per cada paraula contar quants cops apareix en el text. Una versió alternativa d'aquest procés pot ser simplement indicar si apareix o no en el text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dades del New York Times</b>\n",
    "En aquest exemple, el nostre objectiu és classificar automàticament les notícies d'acord al seu titular en vint temes o tòpics. Les dades que disposem són cadascú dels articles de primera plana del New York Times entre 1996 i 2006, classificats segons Policy Agendas ( http://www.policyagendas.org ). Aquesta recollida de dades l'ha compilat n'Amber E. Boydstun\n",
    "\n",
    "Concretament, els tòpics són els següents\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td>\n",
    "1 \n",
    "<td>\n",
    "Macroeconomics\n",
    "<tr>\n",
    "<td>\n",
    "2 \n",
    "<td>\n",
    "Civil Rights, Minority Issues, and Civil Liberties \n",
    "<tr>\n",
    "<td>\n",
    "3\n",
    "<td>\n",
    "Health\n",
    "<tr>\n",
    "<td>\n",
    "4 \n",
    "<td>Agriculture\n",
    "<tr>\n",
    "<td>\n",
    "5 \n",
    "<td>Labor, Employment, and Immigration\n",
    "<tr>\n",
    "<td>\n",
    "6 \n",
    "<td> Education\n",
    "<tr>\n",
    "<td>\n",
    "7\n",
    "<td>Environment\n",
    "<tr>\n",
    "<td>\n",
    "8\n",
    "<td>Energy\n",
    "<tr>\n",
    "<td>\n",
    "10 \n",
    "<td>Transportation\n",
    "<tr>\n",
    "<td>\n",
    "12 \n",
    "<td>Law, Crime, and Family Issues\n",
    "<tr>\n",
    "<td>\n",
    "13 \n",
    "<td>Social Welfare\n",
    "<tr>\n",
    "<td>\n",
    "14 \n",
    "<td>Community Development and Housing Issues\n",
    "<tr>\n",
    "<td>\n",
    "15 \n",
    "<td>Banking, Finance, and Domestic Commerce\n",
    "<tr>\n",
    "<td>\n",
    "16 \n",
    "<td>Defense\n",
    "<tr>\n",
    "<td>\n",
    "17 \n",
    "<td>Space, Science, Technology and Communications\n",
    "<tr>\n",
    "<td>\n",
    "18 \n",
    "<td>Foreign Trade\n",
    "<tr>\n",
    "<td>\n",
    "19 \n",
    "<td>International Affairs and Foreign Aid\n",
    "<tr>\n",
    "<td>\n",
    "20 \n",
    "<td>Government Operations\n",
    "<tr>\n",
    "<td>\n",
    "21 \n",
    "<td>Public Lands and Water Management\n",
    "<tr>\n",
    "<td>\n",
    "24 \n",
    "<td>State and Local Government Administration\n",
    "<tr>\n",
    "<td>\n",
    "26 \n",
    "<td>Weather and Natural Disasters\n",
    "<tr>\n",
    "<td>\n",
    "27 \n",
    "<td>Fires\n",
    "<tr>\n",
    "<td>\n",
    "28 \n",
    "<td>Arts and Entertainment\n",
    "<tr>\n",
    "<td>\n",
    "29 \n",
    "<td>Sports and Recreation\n",
    "<tr>\n",
    "<td>\n",
    "30 \n",
    "<td>Death Notices\n",
    "<tr>\n",
    "<td>\n",
    "31 \n",
    "<td>Churches and Religion\n",
    "<tr>\n",
    "<td>\n",
    "99 \n",
    "<td>Other, Miscellaneous, and Human Interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exemple.</b>\n",
    "Imaginem que tenim 4 notícies que pertanyen només a dues categories y={'8. Energy', '2. Civil Rights, Minority Issues, and Civil Liberties'}, podríem seleccionar les següents paraules per tal de distingir les dues categories x={'nuclear', 'verd', 'ecologia', 'independència', 'autonomia', 'referèndum'}\n",
    "\n",
    "<table border=\"1\">\n",
    "<tr>\n",
    "<td></td>\n",
    "<td>nuclear</td>\n",
    "<td>verd</td>\n",
    "<td>ecologia</td>\n",
    "<td>independència</td>\n",
    "<td>autonomia</td>\n",
    "<td>referèndum</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 1('2. Civil Rights...')</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 2('2. Civil Rights...')</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>3</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 3('8. Energy')</td>\n",
    "<td>1</td>\n",
    "<td>3</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "<td>1</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>news 4('8. Energy')</td>\n",
    "<td>2</td>\n",
    "<td>1</td>\n",
    "<td>2</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "<td>0</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<br>\n",
    "amb aquesta representació la notícia 2, corresponent a la categoria '2. Civil Rights...', quedaria representat pel vector numèric (0,1,0,1,2,3). Si fem servir la representació alternativa (booleana) tindríem (0,1,0,1,1,1) que indica la presència de les paraules. Si la descripció és adient s'espera que les categories es puguin distingir entre elles amb facilitat.\n",
    "<br><br>\n",
    "<b>El classificador Naïve Bayes.</b><br>\n",
    "Un cop tenim una representació necessitem un procés d'aprenentatge que ens permeti passar de la descripció a una categoria. En aquesta pràctica farem servir el classificador Naïve Bayes. Aquest classificador forma part de la família de classificadors probabilístics. La sortida d'un classificador probabilístic és un valor de probabilitat donat un exemple per cadascuna de les categories. La decisió final correspon a la categoria amb més probabilitat. Per exemple, amb la descripció anterior esperem que la sortida sigui de l'estil,<br>\n",
    "$$p( y = 'Civil\\_Rights' \\; | \\; x = (0,1,0,1,1,1)) = 0.6$$\n",
    "$$p( y = 'Energy' \\; | \\; x = (0,1,0,1,1,1)) = 0.4$$\n",
    "\n",
    "<br>\n",
    "Els classificadors probabilistics Bayesians es basen en el teorema de Bayes per realitzar els càlculs per trobar la probabilitat condicionada: <br>\n",
    "\n",
    "$$ p(x,y) = p(x|y)p(y) = p(y|x)p(x)$$\n",
    "<br>\n",
    "d'on podem extreure que: <br>\n",
    "$$ p(y,x) = \\frac{p(x|y)p(y)}{p(x)}$$\n",
    "<br>\n",
    "\n",
    "\n",
    "En molts casos p(y) i p(x) són desconeguts i es consideren equiprobables. Per tant, la\n",
    "decisió es simplifica a:\n",
    "<br>\n",
    "$$ p(y|x) = c · p(x|y)$$\n",
    "\n",
    "<br>\n",
    "Les deduccions fins a aquest punt són vàlides per la majoria de classificadors Bayesians. Naïve Bayes es distingeix de la resta perquè imposa una condició encara més restrictiva. Considerem x=(x1,x2,x3,...,xN) un conjunt d'N variables aleatòries. Naïve Bayes assumeix que totes elles són independents entre elles i per tant podem escriure:\n",
    "<br>\n",
    "$$p(x_1,x_2,...,x_N | y) = p(x_1|y)p(x_2|y)...p(x_N|y)$$\n",
    "\n",
    "<br>\n",
    "Per tant en el nostre cas es pot veure com:\n",
    "<br>\n",
    "$$p(y='Civil\\_Rights' \\; |\\;x=(0,1,0,1,1,1)) = p(x_1=0|\\;'Civil\\_Rights' \\; )p(x_2=1|\\;'Civil\\_Rights' \\; )...p(x_6=1|\\;'Civil\\_Rights' \\; )$$\n",
    "\n",
    "<br>\n",
    "Podem interpretar l'anterior equació de la següent forma: La probabilitat de que el document descrit pel vector de característiques (0,1,0,1,1,1) sigui de la classe \"Civil Rights\" és proporcional al producte de la probabilitat que la primera paraula del vector (nuclear) no aparegui en les notícies sobre \"Civil Rights\"  per la probabilitat que la segona paraula sí que hi aparegui, etc.\n",
    "\n",
    "<br>\n",
    "<b>Estimant les probabilitats marginals condicionades</b>\n",
    "L'últim pas que ens queda és trobar el valor de les probabilitats condicionades. Farem servir la representació de 0's i 1's indicant que la paraula no apareix (0) o sí apareix (1) a la notícia. Per trobar el valor de la probabilitat condicionada farem servir una aproximació freqüentista a la probabilitat. Això vol dir que calcularem la freqüència d'aparició de cada paraula per a cada categoria. Aquest càlcul es fa dividint el nombre de notícies de la categoria en que apareix la paraula pel nombre total de notícies d'aquella categoria. En l'exemple anterior, \n",
    "$p(x2'verd'=1 |y='Civil\\_Rights')=1/2 $\n",
    ", mentres que  $p(x2 'verd' =1 |y='Energy')=2/2 $\n",
    "\n",
    "En gneral:\n",
    "<br>\n",
    "$$p(x = 1 | y = C)= \\frac{A}{B} $$\n",
    "<br>\n",
    "on A és el número de notícies de la categoria C on hi apareix la paraula 'x' i B és el número total de notícies de la categoria C.\n",
    "\n",
    "\n",
    "<b>Punts dèbils: </b><br>\n",
    "\n",
    "<b> * El problema de la probabilitat 0</b>\n",
    "Si us hi fixeu bé, en l'anterior exemple la probabilitat <b>p(x2'verd'= 0 | y='Energy')</b> és 0 !! Això vol dir, que si en la notícia no hi apareix la paraula 'verd' no pot ser classificada com a categoria 'Energy'!! No sembla raonable que s'assigni o no en aquesta categoria segons si en la notícia hi apareix o no una única paraula. Per tant, el que s'acostuma a fer és donar una baixa probabilitat en comptes de zero. Una de les possibles solucions es fer servir la correcció de Laplace. Seguint l'exemple anterior la correcció de Laplace és\n",
    "\n",
    "<br>\n",
    "$$p(x=1 | y = 'C' ) = \\frac{A+1}{B+M}$$ \n",
    "on M és el nombre de catergories\n",
    "\n",
    "<b> * El problema de com escollir el vector de carecterístiques</b>\n",
    "L'elecció de les paraules que formen el vector de característiques és un pas crític. En funció de com de bona sigui aquesta descripció, millor funcionarà el sistema. Tot i que us deixem a vosaltres la política de creació del vector de característiques us donem una d'exemple. Per saber quines paraules fer servir podeu seleccionar de totes les paraules de totes les notícies aquelles que apareixen entre en un 10 i un 50 percent del total (sense tenir en compte la categoria). Podeu experimentar variant aquests valors.\n",
    "\n",
    "<b> * El problema del \"underflow\"</b>\n",
    " La funció que hem de calcular en el Naive Bayes és un producte. El nombre de caractéristiques del vector és el nombre de termes del producte. Aquests nombres són iguals o menors a 1, si els multipliquem tots entre ells el resultat serà massa petit per a representar-lo en un nombre de punt flotant i el càlcul acabarà sent reduït a zero. Per solucionar aquest problema en comptes d'operar fent multiplicacions, se sol passar a l'escala logarítmica i allà operar fent servir sumes en comptes de multiplicacions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_ID</th>\n",
       "      <th>Date</th>\n",
       "      <th>Article_Sequence</th>\n",
       "      <th>Title</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Topic_6digit</th>\n",
       "      <th>Topic_4digit</th>\n",
       "      <th>Topic_2digit</th>\n",
       "      <th>War on Terror</th>\n",
       "      <th>Katrina</th>\n",
       "      <th>Israel/Palestine</th>\n",
       "      <th>Immigration</th>\n",
       "      <th>Presidential Elections</th>\n",
       "      <th>Clinton Impeachment</th>\n",
       "      <th>Enron</th>\n",
       "      <th>Darfur</th>\n",
       "      <th>Race/Ethnicity</th>\n",
       "      <th>Schiavo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>a</td>\n",
       "      <td>Nation's Smaller Jails Struggle To Cope With S...</td>\n",
       "      <td>Jails overwhelmed with hardened criminals</td>\n",
       "      <td>120500</td>\n",
       "      <td>1205</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>b</td>\n",
       "      <td>Dancing (and Kissing) In the New Year</td>\n",
       "      <td>new years activities</td>\n",
       "      <td>280000</td>\n",
       "      <td>2800</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>c</td>\n",
       "      <td>Forbes's Silver Bullet for the Nation's Malaise</td>\n",
       "      <td>Steve Forbes running for President</td>\n",
       "      <td>201201</td>\n",
       "      <td>2012</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>d</td>\n",
       "      <td>Up at Last, Bridge to Bosnia Is Swaying Gatewa...</td>\n",
       "      <td>U.S. military constructs bridge to help their ...</td>\n",
       "      <td>160200</td>\n",
       "      <td>1602</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1/1/1996</td>\n",
       "      <td>e</td>\n",
       "      <td>2 SIDES IN SENATE DISAGREE ON PLAN TO END FURL...</td>\n",
       "      <td>Democrats and Republicans can't agree on plan ...</td>\n",
       "      <td>201206</td>\n",
       "      <td>2012</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article_ID      Date Article_Sequence  \\\n",
       "0           1  1/1/1996                a   \n",
       "1           2  1/1/1996                b   \n",
       "2           3  1/1/1996                c   \n",
       "3           4  1/1/1996                d   \n",
       "4           5  1/1/1996                e   \n",
       "\n",
       "                                               Title  \\\n",
       "0  Nation's Smaller Jails Struggle To Cope With S...   \n",
       "1             Dancing (and Kissing) In the New Year    \n",
       "2   Forbes's Silver Bullet for the Nation's Malaise    \n",
       "3  Up at Last, Bridge to Bosnia Is Swaying Gatewa...   \n",
       "4  2 SIDES IN SENATE DISAGREE ON PLAN TO END FURL...   \n",
       "\n",
       "                                             Summary  Topic_6digit  \\\n",
       "0          Jails overwhelmed with hardened criminals        120500   \n",
       "1                               new years activities        280000   \n",
       "2                 Steve Forbes running for President        201201   \n",
       "3  U.S. military constructs bridge to help their ...        160200   \n",
       "4  Democrats and Republicans can't agree on plan ...        201206   \n",
       "\n",
       "   Topic_4digit  Topic_2digit  War on Terror  Katrina  Israel/Palestine  \\\n",
       "0          1205            12              0        0                 0   \n",
       "1          2800            28              0        0                 0   \n",
       "2          2012            20              0        0                 0   \n",
       "3          1602            16              0        0                 0   \n",
       "4          2012            20              0        0                 0   \n",
       "\n",
       "   Immigration  Presidential Elections  Clinton Impeachment  Enron  Darfur  \\\n",
       "0            0                       0                    0      0       0   \n",
       "1            0                       0                    0      0       0   \n",
       "2            0                       1                    0      0       0   \n",
       "3            0                       0                    0      0       0   \n",
       "4            0                       0                    0      0       0   \n",
       "\n",
       "   Race/Ethnicity  Schiavo  \n",
       "0               0        0  \n",
       "1               0        0  \n",
       "2               0        0  \n",
       "3               0        0  \n",
       "4               0        0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%reset -f\n",
    "#load data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from random import shuffle\n",
    "data=pd.read_csv('Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 1 </b> Escriure una funció <b>count_news(dataframe)</b> que retorni el nombre total de noticies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Retorna el número total de notícies. \n",
    "def count_news(df):\n",
    "    return df.shape[0]\n",
    "#print count_news(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 2:</b> Escriure una funció <b>count_topic_news(dataframe)</b> que compti quantes notícies hi ha per cadascun dels tópics. Agaferem el camp \"Topic_2digit\" com a identificador de tópic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Retorna una Series que compte el número de notícies per a cadascun dels tópics\n",
    "def count_topic_news(df):\n",
    "    return df.groupby('Topic_2digit')['Article_ID'].count()\n",
    "#count_topic_news(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 3:</b> Escriure una funció <b>count_words(dataframe)</b> que retorni un diccionari amb totes les paraules que hi hagi en totes les notícies, indicant per cada paraula quantes ocurrències en total hi ha i en quantes notícies surt.\n",
    "<br>Possible format sortida: {word :  {n_ocur: valor;n_news: valor}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "r = re.compile(r'[\\s{}]+'.format(re.escape(punctuation)))\n",
    "\n",
    "# Aquesta funció ha de construir un diccionari que contingui totes les paraules que s'han trobat indicant \n",
    "# el total de cops que ha aparegut i el nombre de notícies on apareix\n",
    "def count_words(df):\n",
    "    word_dicc=dict()\n",
    "    dfnew = pd.concat([df['Title'],df['Summary']], axis=1, keys=['Title', 'Summary'])\n",
    "    for article in dfnew.iterrows():\n",
    "        article = article[1][0] + ' ' + article[1][1]\n",
    "        article = article.lower()\n",
    "        words = []\n",
    " \n",
    "        for word in [word for word in r.split(article) if word != '']:\n",
    "            if word in word_dicc:\n",
    "                if word not in words: \n",
    "                    word_dicc[word]['n_ocur'] = word_dicc[word]['n_ocur'] + 1\n",
    "                    word_dicc[word]['n_news'] = word_dicc[word]['n_news'] + 1\n",
    "                else: \n",
    "                    word_dicc[word]['n_ocur'] = word_dicc[word]['n_ocur'] + 1\n",
    "            else: \n",
    "                word_dicc[word] = {'n_ocur':1,'n_news':1}\n",
    "            words.append(word)\n",
    "    return word_dicc\n",
    "\n",
    "\n",
    "dicc_text = count_words(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 4:</b> escriure una funció <b>count_words_topic(dataframe,dicc_text)</b> que retorna un diccionari que conté el nombre de cops que ha aparegut  cada paraula i el número de notícies on  ha aparegut. Aquesta informació ha de ser dividida pels diferents tópics de noticies.\n",
    "<br>Possible format sortida: {Topic :  {word :  {n_ocur: valor;n_news: valor} } } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compta la freqüència de les paraules per a un tòpic determinat\n",
    "def count_words_topic(df,dicc_text):\n",
    "    dic = dict()\n",
    "    for topic in df['Topic_2digit'].unique():\n",
    "        dic[topic] = count_words(df[df['Topic_2digit']==topic])\n",
    "    return dic\n",
    "\n",
    "words_topics=count_words_topic(data,dicc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 5:</b> Calcular amb la funció <b>topNword(df,words_topics,N)</b> quines son les N paraules més representatives (les que apareixen amb més freqüència) de cadascun dels tòpics. Retorneu un diccionari amb els següent format: {1: llista_top_words_topic_1; 2: llista_top_words_topic_2;...}\n",
    "<br>Teniu en compte que també haureu de filtrar aquelles paraules que apareixen en la majoria de notícies, així com també, les que únicament apareixen en un conjunt molt petit de notícies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "badwords = ['the','in','to','on','as','by','of','for','over','and','it','is','at','with','but','our','their','he','she','an','what','who','from','are','new']\n",
    "#Calcula les N parules més representativa de cada tòpic . La sortida ha de \n",
    "# ser un diccionari on tenim tantes entrades com tòpics\n",
    "# el valors de les entrades ha de ser una llista amb les paraules seleccionades.\n",
    "def topNwords(df,words_topics,N):\n",
    "    dic = dict()\n",
    "    topWordsTopic = []\n",
    "    for topic in df['Topic_2digit'].unique():\n",
    "        dic[topic] = [selectedWords[0] for selectedWords in sorted(words_topics[topic].items(), key=operator.itemgetter(1),reverse = True) if (selectedWords[0] not in badwords and len(selectedWords[0])!=1)][:N]\n",
    "    return dic\n",
    "\n",
    "#topNwords(data,words_topics,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 6</b>: Creeu el vector de característiques necessari per a fer l’entrenament del Naïve Bayes (funció <b>create_features()</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Crea el vector de característiques necessari per a l'entrenament del classificador Naive Bayes\n",
    "# selected_words: ha de ser el diccionari que retorna topNWords.\n",
    "# data : conté totes les notícies\n",
    "# Rertorna un diccionari que conté un np.array per a cadascuna de les notícies amb el vector de característiques corresponent.\n",
    "\n",
    "def create_features(df,top_words):\n",
    "    dict_feat_vector=dict()\n",
    "    caract_vector = []\n",
    "    for topic in top_words.keys(): caract_vector += top_words[topic]\n",
    "    v = np.unique(caract_vector)\n",
    "    \n",
    "    array = np.zeros(len(v))\n",
    "\n",
    "    \n",
    "    dfnew = pd.concat([df['Title'],df['Summary'],df['Topic_2digit']], axis=1, keys=['Title', 'Summary','Topic_2digit'])\n",
    "    for article in dfnew.iterrows():\n",
    "        t = article[1][2]\n",
    "        article = article[1][0] + ' ' + article[1][1]\n",
    "        article = article.lower()\n",
    "        for word in [word for word in r.split(article) if word != '' and word in v]:\n",
    "            array[np.where(v==word)] = 1\n",
    "        article += \":topic:\" + str(t)\n",
    "        dict_feat_vector[article] = array\n",
    "        array = np.zeros(len(v))\n",
    "                \n",
    "    return dict_feat_vector,v\n",
    "\n",
    "N =100 # Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor. \n",
    "words_topics=count_words_topic(data,dicc_text)\n",
    "top_words=topNwords(data,words_topics,N)\n",
    "dict_feat_vector,vector_words = create_features(data,top_words)\n",
    "#print vector_words,dict_feat_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 7</b>: Implementeu la funció d'aprenentatge del classificador Naïve Bayes (funció <b>naive_bayes_learn()</b>).  La funció ha de mostrar per pantalla el resultat obtingut \n",
    "<br>\n",
    "<b> * L'error d'entrenament</b>\n",
    "L'error d'entrenament es troba calculant el percentatge d'errors que s'obtenen quan es fa el testeig amb les mateixes dades utilizades per fer entrenament (aprenentatge). Aquest error es un valor molt optimista de com funcionarà el clasificador i mai s'ha de prendre com a mesura per comparar clasificadors.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mètode que implementa el clasificador Naive_Bayes. Ha de mostrar el resultat obtingut per pantalla\n",
    "topics_news = count_topic_news(data)\n",
    "import re\n",
    "\n",
    "\n",
    "def memo(f):\n",
    "    class memodict(dict):\n",
    "        def __init__(self, f):\n",
    "            self.f = f\n",
    "        def __call__(self, *args):\n",
    "            return self[args]\n",
    "        def __missing__(self, key):\n",
    "            ret = self[key] = self.f(*key)\n",
    "            return ret\n",
    "    return memodict(f)\n",
    "\n",
    "def prob(topic,word,i):\n",
    "    p = 0.;\n",
    "    if(i == 1.):\n",
    "        if word in words_topics[topic]:\n",
    "            ratio = ((words_topics[topic][word].values()[1]) + 1)/float(topics_news[topic] + 27)\n",
    "        else:\n",
    "            ratio = 1/float(topics_news[topic] + 27)\n",
    "        p += math.log10(ratio)\n",
    "    else:\n",
    "        if word in words_topics[topic]:\n",
    "            ratio = (float(topics_news[topic]) - (words_topics[topic][word].values()[1]) + 1)/float(topics_news[topic] + 27)\n",
    "        else:\n",
    "            ratio = (float(topics_news[topic]) + 1)/float(topics_news[topic] + 27)\n",
    "        p += math.log10(ratio)\n",
    "    return p\n",
    "\n",
    "def naive_bayes(df,feature_vector):\n",
    "    i = 0\n",
    "    \n",
    "    encerts_topic = [0] * 100\n",
    "    total_topic = [0] * 100\n",
    "\n",
    "    for article in feature_vector:\n",
    "        i+=1\n",
    "        probs = [float(\"-inf\")] * 100\n",
    "        for topic in df[\"Topic_2digit\"].unique():\n",
    "            probs[topic] = 0\n",
    "            fv_article = feature_vector[article]\n",
    "            it = np.nditer(fv_article, flags=['c_index'])\n",
    "            while not it.finished:\n",
    "                probs[topic] += prob(topic,vector_words[it.index],float(it[0]))\n",
    "                it.iternext()\n",
    "                \n",
    "        t = int(article.split(\":topic:\")[1])\n",
    "        if probs.index(max(probs)) == t: encerts_topic[t] += 1\n",
    "        total_topic[t] += 1\n",
    "        if i > 100: break\n",
    "    \n",
    "    for topic in  df[\"Topic_2digit\"].unique():\n",
    "        encerts =  encerts_topic[topic]\n",
    "        total = total_topic[topic]\n",
    "        #if(total != 0): print \"Tòpic \" , topic , \" encerts: \" , encerts ,\" / \" , \"Notícies: \" , total ,\" / \", encerts/float(total)*100 , \"%\" \n",
    "        #else : print \"Tòpic \" , topic , \" encerts: \" , encerts ,\" / \" , \"Notícies: \" , total \n",
    "        \n",
    "    \n",
    "    #print \"Error naive bayes: \" , 1-sum(encerts_topic)/float(sum(total_topic))\n",
    "        \n",
    "\n",
    "naive_bayes(data,dict_feat_vector)\n",
    "\n",
    "## EXEMPLE SORTIDA (LES XIFRES SÓN INVENTADES!):\n",
    "#Tòpic 1 encerts: 24 / Notícies: 34 / 70.59 %\n",
    "#Tòpic 2  encerts: 21 / Notícies: 26 / 80.77 %\n",
    "#Tòpic 3  encerts: 29 / Notícies: 32 / 90.62 %\n",
    "#Tòpic 4  encerts: 26 / Notícies: 29 / 89.66 %\n",
    "#\n",
    "#Error naive bayes: 17.36 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Exercici 8: </b> Indiqueu l'error de generalització fent servir *n-fold validation* (funció <b>n_fold ()</b> )\n",
    "\n",
    "**Aproximació a l'error de generalització fent servir  *n-fold validation* **.\n",
    "Una bona forma de veure com funcionaria el nostre classificador davant de dades sobre les quals no s'ha entrenat és fer servir l'estratègia n-fold. Aquesta estratègia testeja el classificador amb una partició de les dades d'entrenament i fa l'entrenament sobre la resta de dades que hem exclòs. Aquest procés d'exclusió es repeteix per cadascún dels *folds* de les dades d'entrenament. El nombre de *folds* determina quantes particions hem de fer, i per tant, les dades que hi ha en el conjut de test. Per exemple, en un *5-fold* validation, es fan 5 particions, les dades de test són un cinquè de les dades i l'entrenament es fa amb els quatre cinquens restants. El percentatge d'errors fent servir aquesta estratègia permet comparar classificadors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mètode per avaluar el classificador mitjançant la tècnica n-fold validation. n és el nombre de folds  \n",
    "#Ha de mostrar per pantalla el resultat obtingut \n",
    "def n_fold(df,feature_vector,n):\n",
    "    df = df.iloc[np.random.permutation(len(df))]\n",
    "    l = count_news(df)\n",
    "    N = 100\n",
    "    for i in xrange(n): \n",
    "        test = df[int(math.floor(i*l/n)):int(math.floor((i+1)*l/n))]\n",
    "        train2 = df[int(math.floor((i+1)*l/n)):]\n",
    "        train1 = df[:int(math.floor(i*l/n))]\n",
    "        train = pd.concat([train1,train2])\n",
    "        \n",
    "        print train2.shape,train1.shape,train.shape\n",
    "        print train\n",
    "        \n",
    "        words_topics=count_words_topic(train,dicc_text)\n",
    "        topic_news=count_topic_news(train)\n",
    "        #top_words=topNwords(train,words_topics,N)\n",
    "        #feature_vector = create_features(test,top_words) \n",
    "        naive_bayes(test,feature_vector)\n",
    "        \n",
    "    return 0\n",
    "\n",
    "n_fold(data,vector_words,5)\n",
    "    \n",
    "## EXEMPLE SORTIDA (LES XIFRES SÓN INVENTADES!):\n",
    "#Tòpic 1 encerts: 24 / Notícies: 34 / 70.59 %\n",
    "#Tòpic 2  encerts: 21 / Notícies: 26 / 80.77 %\n",
    "#Tòpic 3  encerts: 29 / Notícies: 32 / 90.62 %\n",
    "#Tòpic 4  encerts: 26 / Notícies: 29 / 89.66 %\n",
    "#\n",
    "\n",
    "#Error naive bayes: 17.36 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Resultat Final</b>\n",
    "\n",
    "Definició de la funció principal. Modifiqueu la funció per tal que s'ajusti a les vostres funcions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main. Es criden a totes les funcions per a la correcte execució del programa.   \n",
    "def main():\n",
    "\n",
    "    import pandas as pd\n",
    "    data=pd.read_csv('./files/Boydstun_NYT_FrontPage_Dataset_1996-2006_0.csv')\n",
    "    N = 20 # Aquest parametre el podem canviar i fer proves per avaluar quin és el millor valor. \n",
    "    words_topics=count_words_topic(data,dicc_text)\n",
    "    top_words=topNwords(data,words_topics,N)\n",
    "    feature_vectors = create_features(data,top_words) \n",
    "    naive_bayes(data,feature_vectors) #fins aquí error d'entrenament. Fem servir totes les dades.\n",
    "    \n",
    "    \n",
    "    n_fold(data, feature_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
